{"cells":[{"cell_type":"code","execution_count":2,"id":"3116087e","metadata":{},"outputs":[],"source":["from pyspark.sql import SparkSession"]},{"cell_type":"code","execution_count":3,"id":"185253aa","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["Setting default log level to \"WARN\".\n","To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n","25/04/29 02:09:53 INFO SparkEnv: Registering MapOutputTracker\n","25/04/29 02:09:53 INFO SparkEnv: Registering BlockManagerMaster\n","25/04/29 02:09:53 INFO SparkEnv: Registering BlockManagerMasterHeartbeat\n","25/04/29 02:09:53 INFO SparkEnv: Registering OutputCommitCoordinator\n"]}],"source":["# Create a SparkSession instance (an entry point to all Spark functions)\n","spark = SparkSession.builder.appName(\"MYAPP\").getOrCreate()"]},{"cell_type":"code","execution_count":4,"id":"04db2934","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]},{"name":"stdout","output_type":"stream","text":["+---------------+------------+------------+------------+\n","|passenger_count|pulocationid|dolocationid|total_amount|\n","+---------------+------------+------------+------------+\n","|            1.0|       151.0|       239.0|        9.95|\n","|            1.0|       239.0|       246.0|        16.3|\n","|            3.0|       236.0|       236.0|         5.8|\n","|            5.0|       193.0|       193.0|        7.55|\n","|            5.0|       193.0|       193.0|       55.55|\n","|            5.0|       193.0|       193.0|       13.31|\n","|            5.0|       193.0|       193.0|       55.55|\n","|            1.0|       163.0|       229.0|        9.05|\n","|            1.0|       229.0|         7.0|        18.5|\n","|            2.0|       141.0|       234.0|        13.0|\n","+---------------+------------+------------+------------+\n","only showing top 10 rows\n","\n"]}],"source":["df = spark.read.csv('gs://dataproc-staging-us-central1-740540545323-gfe3jrkb/data/2019-01-h1.csv', header=True,\n","    inferSchema=True)\n","df_taxi = df.select(\"passenger_count\", \"pulocationid\", \"dolocationid\", \"total_amount\")\n","df_taxi.show(10)"]},{"cell_type":"code","execution_count":5,"id":"183fab38","metadata":{},"outputs":[],"source":["trainDF, testDF = df_taxi.randomSplit([.8, .2])"]},{"cell_type":"code","execution_count":6,"id":"e85c8f3b","metadata":{},"outputs":[],"source":["from pyspark.ml.feature import VectorAssembler"]},{"cell_type":"code","execution_count":7,"id":"8611da80","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\r","[Stage 3:>                                                          (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+---------------+------------+------------+------------+--------------+\n","|passenger_count|pulocationid|dolocationid|total_amount|      features|\n","+---------------+------------+------------+------------+--------------+\n","|            0.0|         1.0|         1.0|        90.0| [0.0,1.0,1.0]|\n","|            0.0|         1.0|         1.0|      116.75| [0.0,1.0,1.0]|\n","|            0.0|         4.0|         4.0|         4.8| [0.0,4.0,4.0]|\n","|            0.0|         4.0|         4.0|        5.75| [0.0,4.0,4.0]|\n","|            0.0|         4.0|        17.0|        20.3|[0.0,4.0,17.0]|\n","|            0.0|         4.0|        68.0|        15.8|[0.0,4.0,68.0]|\n","|            0.0|         4.0|        79.0|         5.3|[0.0,4.0,79.0]|\n","|            0.0|         4.0|        79.0|         5.8|[0.0,4.0,79.0]|\n","|            0.0|         4.0|        79.0|        6.35|[0.0,4.0,79.0]|\n","|            0.0|         4.0|        79.0|         7.8|[0.0,4.0,79.0]|\n","+---------------+------------+------------+------------+--------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["vecAss = VectorAssembler(inputCols = [\"passenger_count\", \"pulocationid\", \"dolocationid\"], outputCol = \"features\")\n","vecTrainDF = vecAss.transform(trainDF)\n","vecTrainDF.show(10)"]},{"cell_type":"code","execution_count":8,"id":"0fc809e4","metadata":{},"outputs":[],"source":["from pyspark.ml.regression import DecisionTreeRegressor"]},{"cell_type":"code","execution_count":9,"id":"0730cd5e","metadata":{},"outputs":[],"source":["dt = DecisionTreeRegressor(featuresCol=\"features\", labelCol=\"total_amount\")\n","dt = dt.setMaxBins(1000)"]},{"cell_type":"code","execution_count":10,"id":"938a6a00","metadata":{},"outputs":[],"source":["from pyspark.ml import Pipeline"]},{"cell_type":"code","execution_count":11,"id":"829f5217","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["                                                                                \r"]}],"source":["pipeline = Pipeline(stages = [vecAss, dt])\n","pipelineModel = pipeline.fit(trainDF) # ML tranformer DF --> DF + prediction"]},{"cell_type":"code","execution_count":12,"id":"c00bce27","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["\r","[Stage 18:>                                                         (0 + 1) / 1]\r"]},{"name":"stdout","output_type":"stream","text":["+---------------+------------+------------+------------+---------------+------------------+\n","|passenger_count|pulocationid|dolocationid|total_amount|       features|        prediction|\n","+---------------+------------+------------+------------+---------------+------------------+\n","|            0.0|         4.0|         4.0|         4.3|  [0.0,4.0,4.0]|17.841418823273425|\n","|            0.0|         4.0|        33.0|       17.75| [0.0,4.0,33.0]|17.841418823273425|\n","|            0.0|         4.0|        68.0|        12.8| [0.0,4.0,68.0]|17.841418823273425|\n","|            0.0|         4.0|        80.0|       15.95| [0.0,4.0,80.0]|17.841418823273425|\n","|            0.0|         4.0|       137.0|        9.35|[0.0,4.0,137.0]|17.841418823273425|\n","|            0.0|         4.0|       144.0|        9.45|[0.0,4.0,144.0]|17.841418823273425|\n","|            0.0|         7.0|         7.0|        0.31|  [0.0,7.0,7.0]|17.841418823273425|\n","|            0.0|         7.0|         7.0|         3.3|  [0.0,7.0,7.0]|17.841418823273425|\n","|            0.0|         7.0|        43.0|        24.3| [0.0,7.0,43.0]|17.841418823273425|\n","|            0.0|         7.0|       112.0|        16.8|[0.0,7.0,112.0]|17.841418823273425|\n","+---------------+------------+------------+------------+---------------+------------------+\n","only showing top 10 rows\n","\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["predDF = pipelineModel.transform(testDF)\n","predDF.show(10)"]},{"cell_type":"code","execution_count":13,"id":"a452c4b8","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["[Stage 19:=============================>                            (2 + 2) / 4]\r"]},{"name":"stdout","output_type":"stream","text":["33.26615172455253\n"]},{"name":"stderr","output_type":"stream","text":["\r","                                                                                \r"]}],"source":["from pyspark.ml.evaluation import RegressionEvaluator\n","evalr = RegressionEvaluator(predictionCol = 'prediction',\n","                          labelCol = 'total_amount', \n","                          metricName = 'rmse')\n","rmse = evalr.evaluate(predDF)\n","print(rmse)"]},{"cell_type":"code","execution_count":null,"id":"ad31c72f","metadata":{},"outputs":[],"source":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.8"}},"nbformat":4,"nbformat_minor":5}
